{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x4ddy/CC_Coursework/blob/main/Assignment_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f812020",
      "metadata": {
        "id": "2f812020"
      },
      "source": [
        "## Q1. Text normalization, tokenization, stopword removal & frequency\n",
        "\n",
        "Below we:\n",
        "1. Define a 5–6 sentence paragraph on “technology”  \n",
        "2. Convert to lowercase & strip punctuation  \n",
        "3. Tokenize into sentences & words  \n",
        "4. Remove English stopwords  \n",
        "5. Compute and display word frequency (excluding stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0268719",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0268719",
        "outputId": "a70bf70c-bfcb-468f-de6f-75ab8db9be69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')       # For tokenization\n",
        "nltk.download('stopwords')   # For stopword removal\n",
        "nltk.download('wordnet')     # For lemmatization\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce452181",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce452181",
        "outputId": "e63307e6-e03f-4ab6-bed4-31aa30c159cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence tokens: ['\\nin recent years artificial intelligence has revolutionized the way we interact with technology \\nfrom voice assistants responding to casual conversation to self‑driving cars navigating busy streets the impact is profound \\ndevelopers are racing to build smarter algorithms that can learn and adapt in real time \\nmeanwhile concerns around data privacy and ethical use of ai continue to grow \\nit is clear that the next decade will be shaped by breakthroughs in machine learning and neural network research']\n",
            "Word tokens: ['in', 'recent', 'years', 'artificial', 'intelligence', 'has', 'revolutionized', 'the', 'way', 'we', 'interact', 'with', 'technology', 'from', 'voice', 'assistants', 'responding', 'to', 'casual', 'conversation', 'to', 'self‑driving', 'cars', 'navigating', 'busy', 'streets', 'the', 'impact', 'is', 'profound', 'developers', 'are', 'racing', 'to', 'build', 'smarter', 'algorithms', 'that', 'can', 'learn', 'and', 'adapt', 'in', 'real', 'time', 'meanwhile', 'concerns', 'around', 'data', 'privacy', 'and', 'ethical', 'use', 'of', 'ai', 'continue', 'to', 'grow', 'it', 'is', 'clear', 'that', 'the', 'next', 'decade', 'will', 'be', 'shaped', 'by', 'breakthroughs', 'in', 'machine', 'learning', 'and', 'neural', 'network', 'research']\n",
            "Filtered words: ['recent', 'years', 'artificial', 'intelligence', 'revolutionized', 'way', 'interact', 'technology', 'voice', 'assistants', 'responding', 'casual', 'conversation', 'cars', 'navigating', 'busy', 'streets', 'impact', 'profound', 'developers', 'racing', 'build', 'smarter', 'algorithms', 'learn', 'adapt', 'real', 'time', 'meanwhile', 'concerns', 'around', 'data', 'privacy', 'ethical', 'use', 'ai', 'continue', 'grow', 'clear', 'next', 'decade', 'shaped', 'breakthroughs', 'machine', 'learning', 'neural', 'network', 'research']\n",
            "Word Frequency Distribution:\n",
            "recent: 1\n",
            "years: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "revolutionized: 1\n",
            "way: 1\n",
            "interact: 1\n",
            "technology: 1\n",
            "voice: 1\n",
            "assistants: 1\n",
            "responding: 1\n",
            "casual: 1\n",
            "conversation: 1\n",
            "cars: 1\n",
            "navigating: 1\n",
            "busy: 1\n",
            "streets: 1\n",
            "impact: 1\n",
            "profound: 1\n",
            "developers: 1\n",
            "racing: 1\n",
            "build: 1\n",
            "smarter: 1\n",
            "algorithms: 1\n",
            "learn: 1\n",
            "adapt: 1\n",
            "real: 1\n",
            "time: 1\n",
            "meanwhile: 1\n",
            "concerns: 1\n",
            "around: 1\n",
            "data: 1\n",
            "privacy: 1\n",
            "ethical: 1\n",
            "use: 1\n",
            "ai: 1\n",
            "continue: 1\n",
            "grow: 1\n",
            "clear: 1\n",
            "next: 1\n",
            "decade: 1\n",
            "shaped: 1\n",
            "breakthroughs: 1\n",
            "machine: 1\n",
            "learning: 1\n",
            "neural: 1\n",
            "network: 1\n",
            "research: 1\n"
          ]
        }
      ],
      "source": [
        "# Q1: setup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# If running first time, you may need:\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# 1. Define paragraph\n",
        "text = \"\"\"\n",
        "In recent years, artificial intelligence has revolutionized the way we interact with technology.\n",
        "From voice assistants responding to casual conversation to self‑driving cars navigating busy streets, the impact is profound.\n",
        "Developers are racing to build smarter algorithms that can learn and adapt in real time.\n",
        "Meanwhile, concerns around data privacy and ethical use of AI continue to grow.\n",
        "It is clear that the next decade will be shaped by breakthroughs in machine learning and neural network research.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Lowercase & remove punctuation\n",
        "clean = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 3. Tokenize\n",
        "sentences = sent_tokenize(clean)\n",
        "words = word_tokenize(clean)\n",
        "\n",
        "# 4. Remove stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "filtered = [w for w in words if w not in stops and w.isalpha()]\n",
        "\n",
        "# 5. Word frequency\n",
        "freq = Counter(filtered)\n",
        "print(\"Sentence tokens:\", sentences)\n",
        "print(\"Word tokens:\", words)\n",
        "print(\"Filtered words:\", filtered)\n",
        "print(\"Word Frequency Distribution:\")\n",
        "for word, count in freq.most_common():\n",
        "    print(f\"{word}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68a975a",
      "metadata": {
        "id": "e68a975a"
      },
      "source": [
        "## Q2. Stemming vs Lemmatization\n",
        "\n",
        "Using the filtered word list from Q1:\n",
        "1. Porter Stemmer  \n",
        "2. Lancaster Stemmer  \n",
        "3. WordNet Lemmatizer  \n",
        "4. Compare outputs side by side\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3289ab31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3289ab31",
        "outputId": "6e6a82df-76c4-46b0-f8c0-48dfe15fa644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          original        porter     lancaster           lemma\n",
            "0           recent        recent           rec          recent\n",
            "1            years          year          year            year\n",
            "2       artificial      artifici           art      artificial\n",
            "3     intelligence      intellig      intellig    intelligence\n",
            "4   revolutionized    revolution        revolv  revolutionized\n",
            "5              way           way           way             way\n",
            "6         interact      interact      interact        interact\n",
            "7       technology     technolog     technolog      technology\n",
            "8            voice          voic          voic           voice\n",
            "9       assistants        assist        assist       assistant\n",
            "10      responding       respond       respond      responding\n",
            "11          casual        casual           cas          casual\n",
            "12    conversation       convers       convers    conversation\n",
            "13            cars           car           car             car\n",
            "14      navigating         navig         navig      navigating\n",
            "15            busy          busi          busy            busy\n",
            "16         streets        street       streets          street\n",
            "17          impact        impact        impact          impact\n",
            "18        profound      profound      profound        profound\n",
            "19      developers       develop       develop       developer\n",
            "20          racing          race           rac          racing\n",
            "21           build         build         build           build\n",
            "22         smarter       smarter         smart         smarter\n",
            "23      algorithms     algorithm     algorithm       algorithm\n",
            "24           learn         learn         learn           learn\n",
            "25           adapt         adapt         adapt           adapt\n",
            "26            real          real          real            real\n",
            "27            time          time           tim            time\n",
            "28       meanwhile      meanwhil      meanwhil       meanwhile\n",
            "29        concerns       concern       concern         concern\n",
            "30          around        around        around          around\n",
            "31            data          data           dat            data\n",
            "32         privacy       privaci          priv         privacy\n",
            "33         ethical         ethic           eth         ethical\n",
            "34             use           use            us             use\n",
            "35              ai            ai            ai              ai\n",
            "36        continue       continu       continu        continue\n",
            "37            grow          grow          grow            grow\n",
            "38           clear         clear         clear           clear\n",
            "39            next          next          next            next\n",
            "40          decade         decad         decad          decade\n",
            "41          shaped         shape          shap          shaped\n",
            "42   breakthroughs  breakthrough  breakthrough    breakthrough\n",
            "43         machine        machin        machin         machine\n",
            "44        learning         learn         learn        learning\n",
            "45          neural        neural          neur          neural\n",
            "46         network       network       network         network\n",
            "47        research      research      research        research\n"
          ]
        }
      ],
      "source": [
        "# Q2: stemming & lemmatization\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "# If running first time, you may need:\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "results = []\n",
        "for w in filtered:\n",
        "    results.append({\n",
        "        'original': w,\n",
        "        'porter': porter.stem(w),\n",
        "        'lancaster': lancaster.stem(w),\n",
        "        'lemma': lemmatizer.lemmatize(w)\n",
        "    })\n",
        "\n",
        "# Display in a neat table\n",
        "import pandas as pd\n",
        "df2 = pd.DataFrame(results)\n",
        "print(df2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30936af7",
      "metadata": {
        "id": "30936af7"
      },
      "source": [
        "## Q3. Regular Expressions & Text Splitting\n",
        "\n",
        "Starting from the *original* (normalized) text:\n",
        "1. Extract with regex:\n",
        "   - Words > 5 letters  \n",
        "   - Numbers (if any)  \n",
        "   - Capitalized words  \n",
        "2. Using split:\n",
        "   - Words containing only alphabets  \n",
        "   - Words starting with a vowel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59298c63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59298c63",
        "outputId": "b75d226f-26ad-4140-fa5a-e8318cca3733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words >5 letters: ['recent', 'artificial', 'intelligence', 'revolutionized', 'interact', 'technology', 'assistants', 'responding', 'casual', 'conversation', 'driving', 'navigating', 'streets', 'impact', 'profound', 'Developers', 'racing', 'smarter', 'algorithms', 'Meanwhile', 'concerns', 'around', 'privacy', 'ethical', 'continue', 'decade', 'shaped', 'breakthroughs', 'machine', 'learning', 'neural', 'network', 'research']\n",
            "Numbers: []\n",
            "Capitalized words: ['In', 'From', 'Developers', 'Meanwhile', 'It']\n",
            "Alpha-only words: ['In', 'recent', 'years', 'artificial', 'intelligence', 'has', 'revolutionized', 'the', 'way', 'we', 'interact', 'with', 'technology', 'From', 'voice', 'assistants', 'responding', 'to', 'casual', 'conversation', 'to', 'self', 'driving', 'cars', 'navigating', 'busy', 'streets', 'the', 'impact', 'is', 'profound', 'Developers', 'are', 'racing', 'to', 'build', 'smarter', 'algorithms', 'that', 'can', 'learn', 'and', 'adapt', 'in', 'real', 'time', 'Meanwhile', 'concerns', 'around', 'data', 'privacy', 'and', 'ethical', 'use', 'of', 'AI', 'continue', 'to', 'grow', 'It', 'is', 'clear', 'that', 'the', 'next', 'decade', 'will', 'be', 'shaped', 'by', 'breakthroughs', 'in', 'machine', 'learning', 'and', 'neural', 'network', 'research']\n",
            "Words starting with vowel: ['In', 'artificial', 'intelligence', 'interact', 'assistants', 'impact', 'is', 'are', 'algorithms', 'and', 'adapt', 'in', 'around', 'and', 'ethical', 'use', 'of', 'AI', 'It', 'is', 'in', 'and']\n"
          ]
        }
      ],
      "source": [
        "# Q3: regex & splitting\n",
        "import re\n",
        "\n",
        "orig = text  # use the original (with punctuation & case)\n",
        "\n",
        "# a. >5 letters\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', orig)\n",
        "# b. Numbers\n",
        "numbers = re.findall(r'\\d+(?:\\.\\d+)?', orig)\n",
        "# c. Capitalized words\n",
        "capitalized = re.findall(r'\\b[A-Z][a-z]+\\b', orig)\n",
        "\n",
        "# d. Split into alpha-only words\n",
        "alpha_words = re.findall(r'\\b[A-Za-z]+\\b', orig)\n",
        "# e. Words starting with vowel\n",
        "vowel_words = [w for w in alpha_words if re.match(r'^[AEIOUaeiou]', w)]\n",
        "\n",
        "print(\"Words >5 letters:\", long_words)\n",
        "print(\"Numbers:\", numbers)\n",
        "print(\"Capitalized words:\", capitalized)\n",
        "print(\"Alpha-only words:\", alpha_words)\n",
        "print(\"Words starting with vowel:\", vowel_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab47f5cc",
      "metadata": {
        "id": "ab47f5cc"
      },
      "source": [
        "## Q4. Custom Tokenizer & Regex-based Cleaning\n",
        "\n",
        "1. Write a tokenizer that:\n",
        "   - Keeps contractions together (`isn't`)  \n",
        "   - Treats hyphens as part of words (`state-of-the-art`)  \n",
        "   - Separates numbers (but keeps decimals together)  \n",
        "2. Use `re.sub` to:\n",
        "   - Replace emails → `<EMAIL>`  \n",
        "   - Replace URLs → `<URL>`  \n",
        "   - Replace phone nos. (`123-456-7890` or `+91 9876543210`) → `<PHONE>`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd7bf90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fd7bf90",
        "outputId": "4d4ffa4f-ee80-4e81-baad-6ee85086db7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom tokens: ['In', 'recent', 'years', 'artificial', 'intelligence', 'has', 'revolutionized', 'the', 'way', 'we', 'interact', 'with', 'technology', 'From', 'voice', 'assistants', 'responding', 'to', 'casual', 'conversation', 'to', 'self', 'driving', 'cars', 'navigating', 'busy', 'streets', 'the', 'impact', 'is', 'profound', 'Developers', 'are', 'racing', 'to', 'build', 'smarter', 'algorithms', 'that', 'can', 'learn', 'and', 'adapt', 'in', 'real', 'time', 'Meanwhile', 'concerns', 'around', 'data', 'privacy', 'and', 'ethical', 'use', 'of', 'AI', 'continue', 'to', 'grow', 'It', 'is', 'clear', 'that', 'the', 'next', 'decade', 'will', 'be', 'shaped', 'by', 'breakthroughs', 'in', 'machine', 'learning', 'and', 'neural', 'network', 'research']\n"
          ]
        }
      ],
      "source": [
        "# Q4: custom tokenizer & cleaning\n",
        "def custom_tokenize(s):\n",
        "    # placeholder for emails/URLs/phones before tokenization\n",
        "    s = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', s)\n",
        "    s = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', s)\n",
        "    s = re.sub(r'\\+?\\d{1,3}[\\s-]\\d{6,10}', '<PHONE>', s)\n",
        "    s = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: f\"<NUM:{m.group(0)}>\", s)  # mark decimals\n",
        "    s = re.sub(r'\\b\\d+\\b', '<NUM>', s)\n",
        "    # now split on whitespace & punctuation except hyphens/apostrophes\n",
        "    tokens = re.findall(r\"[A-Za-z0-9<>:_]+(?:['-][A-Za-z0-9]+)*\", s)\n",
        "    # restore decimal tokens\n",
        "    return [t.replace('<NUM:', '').replace('>', '') if t.startswith('<NUM:') else t for t in tokens]\n",
        "\n",
        "sample = text.strip()\n",
        "tokens_q4 = custom_tokenize(sample)\n",
        "print(\"Custom tokens:\", tokens_q4)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "my_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}